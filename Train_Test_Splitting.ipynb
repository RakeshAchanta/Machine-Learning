{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e2fd841",
   "metadata": {},
   "source": [
    "# Why do we need train and test samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beb890d",
   "metadata": {},
   "source": [
    "A very common issue when training a model is overfitting. \n",
    "\n",
    "This phenomenon occurs when a model performs really well on the data that we used to train it but it fails to generalise well to new, unseen data points. \n",
    "\n",
    "There are numerous reasons why this can happen — it could be due to the noise in data or it could be that the model learned to predict specific inputs rather than the predictive parameters that could help it make correct predictions. \n",
    "\n",
    "Typically, the higher the complexity of a model the higher the chance that it will be overfitted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943cd126",
   "metadata": {},
   "source": [
    "On the other hand, underfitting occurs when the model has poor performance even on the data that was used to train it. \n",
    "\n",
    "In most cases, underfitting occurs because the model is not suitable for the problem you are trying to solve. \n",
    "\n",
    "Usually, this means that the model is less complex than required in order to learn those parameters that can be proven to be predictive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145e9c15",
   "metadata": {},
   "source": [
    "Creating different data samples for training and testing the model is the most common approach that can be used to identify these sort of issues. \n",
    "\n",
    "In this way, we can use the training set for training our model and then treat the testing set as a collection of data points that will help us evaluate whether the model can generalise well to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55987fa",
   "metadata": {},
   "source": [
    "The simplest way to split the modelling dataset into training and testing sets is to assign 2/3 data points to the former and the remaining one-third to the latter. \n",
    "\n",
    "Therefore, we train the model using the training set and then apply the model to the test set. In this way, we can evaluate the performance of our model. \n",
    "\n",
    "For instance, if the training accuracy is extremely high while the testing accuracy is poor then this is a good indicator that the model is probably overfitted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43e04fc",
   "metadata": {},
   "source": [
    "Note that splitting the dataset into training and testing sets is not the only action that could be required in order to avoid phenomenons such as overfitting. \n",
    "\n",
    "For instance, if both the training and testing sets contain patterns that do not exist in real world data then the model would still have poor performance even though we wouldn’t be able to observe it from the performance evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62d1fdd",
   "metadata": {},
   "source": [
    "On a second note, you should be aware that there are certain situations you should consider creating an extra set called the validation set. \n",
    "\n",
    "The validation set is usually required when apart from model performance we also need to choose among many models and evaluate which model performs better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad658bc9",
   "metadata": {},
   "source": [
    "# How to split our dataset into train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2806e4",
   "metadata": {},
   "source": [
    "In this section, we are going to explore 2 different ways one can use to create training and testing sets. \n",
    "\n",
    "Before jumping into these approaches, let’s create a dummy dataset that will use for demonstration purposes. \n",
    "\n",
    "In the examples below, we will assume that we have a dataset stored in memory as a pandas DataFrame. \n",
    "\n",
    "The iris dataset contains 150 data points, each of which has four features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53c7c2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20d1fb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0                  5.1               3.5                1.4               0.2\n",
      "1                  4.9               3.0                1.4               0.2\n",
      "2                  4.7               3.2                1.3               0.2\n",
      "3                  4.6               3.1                1.5               0.2\n",
      "4                  5.0               3.6                1.4               0.2\n",
      "..                 ...               ...                ...               ...\n",
      "145                6.7               3.0                5.2               2.3\n",
      "146                6.3               2.5                5.0               1.9\n",
      "147                6.5               3.0                5.2               2.0\n",
      "148                6.2               3.4                5.4               2.3\n",
      "149                5.9               3.0                5.1               1.8\n",
      "\n",
      "[150 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "iris_data = load_iris()\n",
    "df = pd.DataFrame(iris_data.data, columns=iris_data.feature_names)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b081f338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.057333</td>\n",
       "      <td>3.758000</td>\n",
       "      <td>1.199333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.435866</td>\n",
       "      <td>1.765298</td>\n",
       "      <td>0.762238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
       "count         150.000000        150.000000         150.000000   \n",
       "mean            5.843333          3.057333           3.758000   \n",
       "std             0.828066          0.435866           1.765298   \n",
       "min             4.300000          2.000000           1.000000   \n",
       "25%             5.100000          2.800000           1.600000   \n",
       "50%             5.800000          3.000000           4.350000   \n",
       "75%             6.400000          3.300000           5.100000   \n",
       "max             7.900000          4.400000           6.900000   \n",
       "\n",
       "       petal width (cm)  \n",
       "count        150.000000  \n",
       "mean           1.199333  \n",
       "std            0.762238  \n",
       "min            0.100000  \n",
       "25%            0.300000  \n",
       "50%            1.300000  \n",
       "75%            1.800000  \n",
       "max            2.500000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b887cd",
   "metadata": {},
   "source": [
    "# Using pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7895d2",
   "metadata": {},
   "source": [
    "The first option is to use pandas DataFrames’ method sample():\n",
    "\n",
    "Return a random sample of items from an axis of object.\n",
    "You can use random_state for reproducibility\n",
    "\n",
    "We initially create the training set by taking a sample with a fraction of 0.8 from the overall rows in the pandas DataFrame. \n",
    "\n",
    "Note that we also define random_state which corresponds to the seed, so that results are reproducible. \n",
    "\n",
    "Subsequently, we create the testing set by simply dropping the corresponding indices from the original DataFrame which are now included in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d877f9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of training examples: 120\n",
      "No. of testing examples: 30\n"
     ]
    }
   ],
   "source": [
    "training_data = df.sample(frac=0.8, random_state=25)\n",
    "testing_data = df.drop(training_data.index)\n",
    "\n",
    "print(f\"No. of training examples: {training_data.shape[0]}\")\n",
    "print(f\"No. of testing examples: {testing_data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf73fa33",
   "metadata": {},
   "source": [
    "# Using scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecafc4fb",
   "metadata": {},
   "source": [
    "The second option — and probably the most commonly used — is the use of sklearn ‘s method called train_test_split():\n",
    "\n",
    "Split arrays or matrices into random train and test subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399e8f84",
   "metadata": {},
   "source": [
    "We can create both the training and testings sets in a one-liner by passing to train_test_split() the modelling DataFrame along with the fraction of the examples that should be included in the testing set. \n",
    "\n",
    "As before, we also set a random_state so that the results are reproducible, that is every time we run the code, the same instances will be included in the training and testing sets respectively. \n",
    "\n",
    "The method returns a tuple with two DataFrames containing the training and testing examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b713571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of training examples: 120\n",
      "No. of testing examples: 30\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "training_data, testing_data = train_test_split(df, test_size=0.2, random_state=25)\n",
    "\n",
    "print(f\"No. of training examples: {training_data.shape[0]}\")\n",
    "print(f\"No. of testing examples: {testing_data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d2b6ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
